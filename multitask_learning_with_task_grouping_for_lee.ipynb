{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEE - Latent Entities Extraction\n",
    "# Multitask Learning with Task-Grouping model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "DATA_PATH = './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'preprocessed_train.csv'))\n",
    "dev_df = pd.read_csv(os.path.join(DATA_PATH, 'preprocessed_dev.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATA_PATH, 'preprocessed_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for curr_df in[train_df, dev_df, test_df]:\n",
    "    curr_df.reactants = curr_df.reactants.apply(eval)\n",
    "    curr_df.products = curr_df.products.apply(eval)\n",
    "    curr_df.entities = curr_df.entities.apply(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining pre-defined entities list to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entities_histogram = pd.Series([e for ents in train_df.entities for e in ents]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 40\n",
    "classes = list(entities_histogram.head(NUM_CLASSES).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['atp', 'adp', 'h2o', 'pi', 'h+', 'o2', 'nadph', 'nadp+', 'coa-sh', 'ppi', 'gtp', 'gdp', 'adomet', 'ub', 'adohcy', 'co2', 'nad+', 'amp', 'nadh', '2og', 'na+', 'l-glu', 'pi(4,5)p2', 'ac-coa', 'ube2i', 'pi(3,4,5)p3', 'lcfa(-)', 'h2o2', 'udp', 'succa', 'p-s15,s20-tp53 tetramer', 'p21 ras:gtp', 'ca2+', 'p21 ras:gdp', 'sumo1:c93-ube2i', 'grb2-1:sos1', 'dag', 'runx2', 'pap', 'ch2o']\n"
     ]
    }
   ],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load word embedding\n",
    "Source & Pre-trained embeddings: https://github.com/cambridgeltl/BioNLP-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(DATA_PATH, 'bio_nlp_vec/PubMed-shuffle-win-30.bin'), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence, hashing_trick\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effective vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 24244\n"
     ]
    }
   ],
   "source": [
    "vocab = [text_to_word_sequence(n) for n in train_df.notes.values]\n",
    "vocab = {x for l in vocab for x in l}\n",
    "print('Vocabulary size:', len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Sequences & Handilng UNKNOWN tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "embeddings = []\n",
    "word2idx[\"PADDING_TOKEN\"] = len(word2idx)\n",
    "vector = np.zeros(w2v.vector_size) #Zero vector vor 'PADDING' word\n",
    "embeddings.append(vector)\n",
    "\n",
    "word2idx[\"UNKNOWN_TOKEN\"] = len(word2idx) - 1\n",
    "vector = np.random.uniform(-0.25, 0.25, w2v.vector_size)\n",
    "embeddings.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for w in w2v.vocab:\n",
    "    word = w.lower()\n",
    "    if word in word2idx or word not in vocab:\n",
    "        continue\n",
    "        \n",
    "    word2idx[word] = len(word2idx) - 1\n",
    "    embeddings.append(w2v[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing for sequences generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hash_word(w):\n",
    "    try:\n",
    "        return word2idx[w]\n",
    "    except KeyError:\n",
    "        return word2idx['UNKNOWN_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert textual passages into a embedding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_one_hot = [hashing_trick(d, vocab_size, hash_function=hash_word) for d in train_df.notes.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len_seq = max([len(x) for x in encoded_one_hot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "padded_notes = pad_sequences(encoded_one_hot, maxlen=max_len_seq, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_seq(curr_df, max_len=max_len_seq):\n",
    "    vocab_size = len(word2idx)\n",
    "    encoded_one_hot = [hashing_trick(d, vocab_size, hash_function=hash_word) for d in curr_df.notes.values]\n",
    "    padded_notes = pad_sequences(encoded_one_hot, maxlen=max_len, padding='post')\n",
    "    return padded_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Co-Occurence Matrix for Task-Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurences = train_df.entities.apply(lambda e: [1 if c in e else 0 for c in classes])\n",
    "occurences = np.array([np.array(o) for o in occurences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cooccurrence_matrix = occurences.T.dot(occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "g = sp.diags(1./cooccurrence_matrix.diagonal())\n",
    "normalized_cooccurence = g * cooccurrence_matrix # normalized co-occurence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UnionFind:\n",
    "    \"\"\"Weighted quick-union with path compression.\n",
    "    The original Java implementation is introduced at\n",
    "    https://www.cs.princeton.edu/~rs/AlgsDS07/01UnionFind.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self._id = list(range(n))\n",
    "        self._sz = [1] * n\n",
    "\n",
    "    def _root(self, i):\n",
    "        j = i\n",
    "        while (j != self._id[j]):\n",
    "            self._id[j] = self._id[self._id[j]]\n",
    "            j = self._id[j]\n",
    "        return j\n",
    "\n",
    "    def find(self, p, q):\n",
    "        return self._root(p) == self._root(q)\n",
    "\n",
    "    def union(self, p, q):\n",
    "        i = self._root(p)\n",
    "        j = self._root(q)\n",
    "        if (self._sz[i] < self._sz[j]):\n",
    "            self._id[i] = j\n",
    "            self._sz[j] += self._sz[i]\n",
    "        else:\n",
    "            self._id[j] = i\n",
    "            self._sz[i] += self._sz[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "uf = UnionFind(NUM_CLASSES)\n",
    "threshold = 0.65\n",
    "for i1,c1 in enumerate(classes):\n",
    "    for i2,c2 in enumerate(classes):\n",
    "        if normalized_cooccurence[i1,i2] > threshold or normalized_cooccurence[i2,i1] > threshold:\n",
    "            uf.union(i1, i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(NUM_CLASSES):\n",
    "    count_together = sum([1 if uf.find(i, j) else 0 for j in range(NUM_CLASSES)]) - 1\n",
    "    if count_together == 0 and sorted(normalized_cooccurence[i], reverse=True)[1] > threshold / 2:\n",
    "        uf.union(i, normalized_cooccurence[i].argsort()[::-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {c: classes[uf._root(classes.index(c))] for c in classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = {}\n",
    "for key, value in sorted(d.items()):\n",
    "    v.setdefault(value, []).append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2og', 'ch2o', 'co2', 'h2o2', 'o2', 'succa']\n",
      "['ac-coa', 'coa-sh']\n",
      "['adohcy', 'adomet']\n",
      "['adp', 'amp', 'atp', 'pi(3,4,5)p3', 'pi(4,5)p2', 'ppi']\n",
      "['ca2+']\n",
      "['dag', 'h2o', 'lcfa(-)', 'pi']\n",
      "['gdp', 'gtp', 'p21 ras:gdp', 'p21 ras:gtp']\n",
      "['grb2-1:sos1']\n",
      "['h+', 'nad+', 'nadh', 'nadp+', 'nadph']\n",
      "['l-glu']\n",
      "['na+']\n",
      "['p-s15,s20-tp53 tetramer']\n",
      "['pap']\n",
      "['runx2']\n",
      "['sumo1:c93-ube2i', 'ube2i']\n",
      "['ub']\n",
      "['udp']\n"
     ]
    }
   ],
   "source": [
    "for x in v.values():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tasks = list(v.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task groups are:\n",
      "['2og', 'ch2o', 'co2', 'h2o2', 'o2', 'succa']\n",
      "['ac-coa', 'coa-sh']\n",
      "['adohcy', 'adomet']\n",
      "['adp', 'amp', 'atp', 'pi(3,4,5)p3', 'pi(4,5)p2', 'ppi']\n",
      "['ca2+']\n",
      "['dag', 'h2o', 'lcfa(-)', 'pi']\n",
      "['gdp', 'gtp', 'p21 ras:gdp', 'p21 ras:gtp']\n",
      "['grb2-1:sos1']\n",
      "['h+', 'nad+', 'nadh', 'nadp+', 'nadph']\n",
      "['l-glu']\n",
      "['na+']\n",
      "['p-s15,s20-tp53 tetramer']\n",
      "['pap']\n",
      "['runx2']\n",
      "['sumo1:c93-ube2i', 'ube2i']\n",
      "['ub']\n",
      "['udp']\n"
     ]
    }
   ],
   "source": [
    "print('Task groups are:')\n",
    "for x in tasks:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of task-groups: 17\n"
     ]
    }
   ],
   "source": [
    "print('Number of task-groups:', len(tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define X & Y for training, validating and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def target_classes(dataframe):\n",
    "    def single_entry(entry, task_entities):\n",
    "        return np.array([1 if c in entry else 0 for c in task_entities])\n",
    "\n",
    "    def single_task(task_entities):\n",
    "        return np.array([single_entry(entry, task_entities) for entry in dataframe.entities])\n",
    "    \n",
    "    return [single_task(task) for task in tasks]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, dev_x, test_x = extract_seq(train_df), extract_seq(dev_df), extract_seq(test_df)\n",
    "train_y, dev_y, test_y = target_classes(train_df), target_classes(dev_df), target_classes(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def mcor(y_true, y_pred):\n",
    "     #matthews_correlation\n",
    "     y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "     y_pred_neg = 1 - y_pred_pos\n",
    " \n",
    "     y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "     y_neg = 1 - y_pos\n",
    " \n",
    "     tp = K.sum(y_pos * y_pred_pos)\n",
    "     tn = K.sum(y_neg * y_pred_neg)\n",
    " \n",
    "     fp = K.sum(y_neg * y_pred_pos)\n",
    "     fn = K.sum(y_pos * y_pred_neg)\n",
    " \n",
    " \n",
    "     numerator = (tp * tn - fp * fn)\n",
    "     denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "     return numerator / (denominator + K.epsilon())\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multitask_loss(y_true, y_pred):\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    return K.mean(K.sum(- y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Embedding, Bidirectional, GRU, Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_mtl_bi_gru_task_grouping():\n",
    "    inputs = Input(shape=(max_len_seq,), dtype='int32')\n",
    "    embedded = Embedding(input_length=max_len_seq, input_dim=embeddings.shape[0],\n",
    "                         output_dim=embeddings.shape[1],\n",
    "                         weights=[embeddings], trainable=False)(inputs)\n",
    "    lstm_out = Bidirectional(\n",
    "        GRU(units=200, activation='sigmoid', dropout=0.50, recurrent_dropout=0.25, return_sequences=False))(\n",
    "        embedded)\n",
    "\n",
    "    output_tasks = [Dense(len(task), activation='sigmoid', name='layer_'+str(i))(lstm_out) for i, task in enumerate(tasks)]\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=output_tasks)\n",
    "    model.compile(loss=[multitask_loss] * len(tasks), optimizer= \"adam\", metrics=['accuracy', f1, precision, recall, mcor])\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_model = model_mtl_bi_gru_task_grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 830)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 830, 200)     4325000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 400)          481200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_0 (Dense)                 (None, 6)            2406        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_1 (Dense)                 (None, 2)            802         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_2 (Dense)                 (None, 2)            802         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_3 (Dense)                 (None, 6)            2406        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_4 (Dense)                 (None, 1)            401         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_5 (Dense)                 (None, 4)            1604        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_6 (Dense)                 (None, 4)            1604        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_7 (Dense)                 (None, 1)            401         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_8 (Dense)                 (None, 5)            2005        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_9 (Dense)                 (None, 1)            401         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_10 (Dense)                (None, 1)            401         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_11 (Dense)                (None, 1)            401         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_12 (Dense)                (None, 1)            401         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_13 (Dense)                (None, 1)            401         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_14 (Dense)                (None, 2)            802         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_15 (Dense)                (None, 1)            401         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_16 (Dense)                (None, 1)            401         bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 4,822,240\n",
      "Trainable params: 497,240\n",
      "Non-trainable params: 4,325,000\n",
      "__________________________________________________________________________________________________\n",
      "number of layers: 20\n"
     ]
    }
   ],
   "source": [
    "model = current_model()\n",
    "model_name = current_model.__name__\n",
    "print('number of layers:', len(model.layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "print('CURRENT MODEL:', model_name)\n",
    "filepath = model_name + \"_weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(train_x, train_y, epochs=350, batch_size=128, callbacks=callbacks_list, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Load weights for trained model\n",
    "Note that as a result of float-numbers precision errors, the calculated co-occurence matrix may be differed in different executions. \n",
    "Therefore, in case you train and then evaluate in two different stages (different machines/sessions), make sure that the task-group division is equivalent in both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('filename')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = target_classes(test_df)\n",
    "y_proba = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = np.concatenate(y_true, axis=1)\n",
    "y_proba = np.concatenate(y_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "y_predict = np.where(y_proba > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for avg_metric in ['micro', 'macro']:\n",
    "    p = precision_score(y_true, y_predict , average=avg_metric)\n",
    "    r = recall_score(y_true, y_predict , average=avg_metric)\n",
    "    f1r = f1_score(y_true, y_predict , average=avg_metric)\n",
    "    results += [avg_metric, p, r, f1r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_proba.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "print(roc_auc[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interp\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(NUM_CLASSES)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(NUM_CLASSES):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= NUM_CLASSES\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "print(roc_auc[\"macro\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.3f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.3f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
